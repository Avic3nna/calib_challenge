{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading raw video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1196\n"
     ]
    }
   ],
   "source": [
    "labeled_videos = glob.glob('./labeled/*.hevc')\n",
    "video_index = 4\n",
    "\n",
    "cap = cv2.VideoCapture(labeled_videos[video_index])\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if(ret == False):\n",
    "        break\n",
    "    else:\n",
    "        cv2.imshow('frame',frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(frame_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Visualising GT pitch and yaw (respectively in 2D array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_gaze(image_in, cam_pos, pitchyaw, length=40.0, thickness=2, color=(255, 0, 0)):\n",
    "    \"\"\"Draw gaze angle on given image with a given cam positions.\"\"\"\n",
    "    image_out = image_in\n",
    "    if len(image_out.shape) == 2 or image_out.shape[2] == 1:\n",
    "        image_out = cv2.cvtColor(image_out, cv2.COLOR_GRAY2BGR)\n",
    "    dx = -length * np.sin(pitchyaw[1])\n",
    "    dy = -length * np.sin(pitchyaw[0])\n",
    "    cv2.arrowedLine(image_out, tuple(np.round(cam_pos).astype(np.int32)),\n",
    "                   tuple(np.round([cam_pos[0] + dx, cam_pos[1] + dy]).astype(int)), color,\n",
    "                   thickness, cv2.LINE_AA, tipLength=0.3)\n",
    "    return image_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_orientation(frame, current_p_y):\n",
    "    pitch = 'Pitch: ' + str(current_p_y[0])\n",
    "    yaw = 'Yaw:  ' + str(current_p_y[1])\n",
    "    cv2.putText(frame, pitch, (20,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 1, 2)\n",
    "    cv2.putText(frame, yaw, (20,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 1, 2)\n",
    "    \n",
    "    if(not math.isnan(current_p_y[0])):\n",
    "        draw_gaze(frame, (582, 845), current_p_y, length=3000, thickness = 15)\n",
    "        draw_gaze(frame, (582, 845), (0,0), length=3000, thickness = 15, color = (255,255,255)) #reference, perfect calib\n",
    "        \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_contour_bad(c):\n",
    "\t# approximate the contour\n",
    "\tperi = cv2.arcLength(c, True)\n",
    "\tapprox = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "\t# the contour is 'bad' if it is not a rectangle\n",
    "\treturn not len(approx) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time lane edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "labeled_videos = glob.glob('./labeled/*.hevc')\n",
    "gt_pitch_yaw = glob.glob('./labeled/*.txt')\n",
    "video_index = 0\n",
    "\n",
    "\n",
    "pitch_yaw = []\n",
    "\n",
    "\n",
    "with open(gt_pitch_yaw[video_index], 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        pitch_yaw.append(float(x) for x in line.split(' '))\n",
    "\n",
    "#frame size (874, 1164, 3)\n",
    "cap = cv2.VideoCapture(labeled_videos[video_index])\n",
    "frame_count = 0\n",
    "\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    \n",
    "    # if(frame_count == 30):\n",
    "    #     cv2.imwrite('sample_img.png', frame)\n",
    "    \n",
    "    try:\n",
    "        current_p_y = list(pitch_yaw[frame_count])\n",
    "        \n",
    "        ### comment the following for normal video\n",
    "        frame = frame[450:700, 400:850]\n",
    "        frame = cv2.GaussianBlur(frame, (13,13), 2)\n",
    "        frame = cv2.Canny(frame, 0, 20)\n",
    "        \n",
    "        # find contours in the image and initialize the mask that will be\n",
    "        # used to remove the bad contours\n",
    "        cnts = cv2.findContours(frame.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = imutils.grab_contours(cnts)\n",
    "        mask = np.ones(frame.shape[:2], dtype=\"uint8\") * 255\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # if the contour is bad, draw it on the mask\n",
    "            if is_contour_bad(c):\n",
    "                cv2.drawContours(mask, [c], -1, 0, -1)\n",
    "        # remove the contours from the image and show the resulting images\n",
    "        frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "        ##\n",
    "    except:\n",
    "        next\n",
    "    \n",
    "    if(ret == False):\n",
    "        break\n",
    "    else:\n",
    "        plot_frame = draw_orientation(frame, current_p_y)\n",
    "        cv2.imshow('frame',plot_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(frame_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following interesting points using Lucas Kanade optical flow and Shi-Tomasi feature tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No frames grabbed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "cap = cv2.VideoCapture(labeled_videos[3])\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15, 15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "# Create some random colors\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "# Take first frame and find corners in it\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "#Shi-Tomasi\n",
    "p0 = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        break\n",
    "    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # calculate optical flow\n",
    "    p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "    # Select good points\n",
    "    if p1 is not None:\n",
    "        good_new = p1[st==1]\n",
    "        good_old = p0[st==1]\n",
    "    # draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        mask = cv.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "        frame = cv.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    "    img = cv.add(frame, mask)\n",
    "    cv.imshow('frame', img)\n",
    "    k = cv.waitKey(1) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    # Now update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting edge lanes on single img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_img_gr = cv2.imread('sample_img.png', cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "blurry_img = cv2.GaussianBlur(raw_img_gr, (5,5), 1)\n",
    "edges = cv2.Canny(blurry_img, 0, 100)\n",
    "\n",
    "cv2.imshow('img',edges)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2f780d3fa0f233da220727534c26375caac5692cb408a3c2164f85e81e52cd5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
